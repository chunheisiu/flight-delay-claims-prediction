{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flight Delay Claims Prediction â€“ Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the source code of the modeling process of the Flight Delay Claims Prediction project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CSV file is first loaded as a pandas data frame, which allows it to be read and manipulated easily. We can show the first 10 lines of the CSV file to examine the schema and some sample rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = pd.read_csv('../data/flight_delays_data.csv', parse_dates=['flight_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thought process of the following data cleaning procedures has been covered in the EDA notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill in missing airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights.loc[flights.Airline.isnull(), 'Airline'] = flights.loc[flights.Airline.isnull(), 'flight_no'].str[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate canceled flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['is_cancelled'] = np.where(flights.delay_time == 'Cancelled', 1, 0)\n",
    "flights.loc[flights.delay_time == 'Cancelled', 'delay_time'] = max(3, float(flights.loc[flights.delay_time != 'Cancelled', 'delay_time'].max()))\n",
    "flights.delay_time = flights.delay_time.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thought process of the following feature engineering procedures has been covered in the EDA notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year, month, day, and day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights['flight_date_year'] = flights.flight_date.dt.year\n",
    "flights['flight_date_month'] = flights.flight_date.dt.month\n",
    "flights['flight_date_day'] = flights.flight_date.dt.day\n",
    "flights['flight_date_dow'] = flights.flight_date.dt.dayofweek\n",
    "\n",
    "flights.Week = flights.Week.astype('category')\n",
    "flights.flight_date_year = flights.flight_date_year.astype('category')\n",
    "flights.flight_date_month = flights.flight_date_month.astype('category')\n",
    "flights.flight_date_day = flights.flight_date_day.astype('category')\n",
    "flights.flight_date_dow = flights.flight_date_dow.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hong Kong public holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_years = flights.flight_date_year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hk_holiday_data(years):\n",
    "    public_holidays = list()\n",
    "\n",
    "    for year in years:\n",
    "        holiday_url = f'https://www.gov.hk/en/about/abouthk/holiday/{year}.htm'\n",
    "        r = requests.get(holiday_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        holiday_dates = pd.read_html(r.text, skiprows=1)[0][1].apply(lambda x: f'{x} {year}')\n",
    "        holiday_dates = pd.to_datetime(holiday_dates, infer_datetime_format=True)\n",
    "        public_holidays.extend(holiday_dates)\n",
    "    \n",
    "    return public_holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_holidays = get_hk_holiday_data(flight_years)\n",
    "flights['is_public_holiday'] = np.where(flights.flight_date.isin(public_holidays), 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hong Kong weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(years):\n",
    "    weather_list = list()\n",
    "\n",
    "    for year in years:\n",
    "        weather_url = f'https://www.hko.gov.hk/cis/dailyExtract/dailyExtract_{year}'\n",
    "        r = requests.get(weather_url, params={'y': year})\n",
    "        weather_data = r.json()['stn']['data']\n",
    "\n",
    "        for elem_month in weather_data:\n",
    "            month = elem_month['month']\n",
    "            day_data = elem_month['dayData'][:-2]\n",
    "            for elem_day in day_data:\n",
    "                day = elem_day[0]\n",
    "                mean_pressure = float(elem_day[1])\n",
    "                mean_temp = float(elem_day[3])\n",
    "                mean_dew_point = float(elem_day[5])\n",
    "                mean_humidity = float(elem_day[6])\n",
    "                mean_cloud = float(elem_day[7])\n",
    "                mean_rainfall = float(elem_day[8]) if elem_day[8] != 'Trace' else 0.0\n",
    "                weather_list.append({'flight_date': pd.to_datetime(f'{year}-{month:02}-{day}'),\n",
    "                                     'mean_pressure': mean_pressure, 'mean_temp': mean_temp, 'mean_dew_point': mean_dew_point,\n",
    "                                     'mean_humidity': mean_humidity, 'mean_cloud': mean_cloud, 'total_rainfall': mean_rainfall})\n",
    "\n",
    "    weather_df = pd.DataFrame(weather_list)\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = get_weather_data(flight_years)\n",
    "flights = pd.merge(flights, weather_df, on='flight_date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert departure hour to category\n",
    "flights.std_hour = flights.std_hour.astype('category')\n",
    "\n",
    "# Drop unecessary flight_date and flight_id column\n",
    "flights = flights.drop(columns=['flight_date', 'flight_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is likely that `delay_time`, `is_cancelled` and `is_claim` would not exist in the hidden set, and the fact that the goal of the project is to predict `delay_time` and derive `is_claim`, `is_cancelled` and `is_claim` will be removed from the training set. Since the `delay_time` column contains enough information for us to derive `is_claim`, we can safely proceed without `is_cancelled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.drop(columns=['is_cancelled', 'is_claim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 899114 entries, 0 to 899113\n",
      "Data columns (total 18 columns):\n",
      " #   Column             Non-Null Count   Dtype   \n",
      "---  ------             --------------   -----   \n",
      " 0   flight_no          899114 non-null  object  \n",
      " 1   Week               899114 non-null  category\n",
      " 2   Departure          899114 non-null  object  \n",
      " 3   Arrival            899114 non-null  object  \n",
      " 4   Airline            899114 non-null  object  \n",
      " 5   std_hour           899114 non-null  category\n",
      " 6   delay_time         899114 non-null  float64 \n",
      " 7   flight_date_year   899114 non-null  category\n",
      " 8   flight_date_month  899114 non-null  category\n",
      " 9   flight_date_day    899114 non-null  category\n",
      " 10  flight_date_dow    899114 non-null  category\n",
      " 11  is_public_holiday  899114 non-null  int64   \n",
      " 12  mean_pressure      899114 non-null  float64 \n",
      " 13  mean_temp          899114 non-null  float64 \n",
      " 14  mean_dew_point     899114 non-null  float64 \n",
      " 15  mean_humidity      899114 non-null  float64 \n",
      " 16  mean_cloud         899114 non-null  float64 \n",
      " 17  total_rainfall     899114 non-null  float64 \n",
      "dtypes: category(6), float64(7), int64(1), object(4)\n",
      "memory usage: 94.3+ MB\n"
     ]
    }
   ],
   "source": [
    "flights.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_features = flights.drop(columns=['delay_time'])\n",
    "cat_cols_idx = [flights_features.columns.get_loc(c) for c in list(flights_features.select_dtypes(exclude=[np.number]).columns)]\n",
    "num_cols_idx = [flights_features.columns.get_loc(c) for c in list(flights_features.select_dtypes(include=[np.number]).columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to proceed with the modeling stage using a PyTorch 2-layer fully connected neural network model.\n",
    "\n",
    "The following code is modified from this notebook by Yannet Interian: https://github.com/yanneta/deep-learning-with-pytorch/blob/master/lesson2-tabular.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, target, seed=1):\n",
    "    \"\"\"\n",
    "    Splits dataset into training and validation sets\n",
    "    \"\"\"\n",
    "    X = np.array(dataset.drop(columns=[target]))\n",
    "    y = np.array(dataset[target])\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    return train_X, valid_X, train_y, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = split_dataset(flights, 'delay_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat_variables(x, help_dict = None):\n",
    "    \"\"\"\n",
    "    Encodes a categorical variable.\n",
    "    The index 0 is left for values not in training.\n",
    "    \"\"\"\n",
    "    uniqs = np.unique(x)\n",
    "    if help_dict is None: help_dict = {v: k + 1 for k, v in enumerate(uniqs)}\n",
    "    levels = len(help_dict.keys()) + 1\n",
    "    x_t = np.array([help_dict.get(x_i, 0) for x_i in x])\n",
    "    return x_t, help_dict, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(train_X, valid_X, cat_ind, num_ind):\n",
    "    \"\"\"\n",
    "    Transform the dataset by encoding features.\n",
    "    \"\"\"    \n",
    "    train_X_cat = train_X[:, cat_ind]\n",
    "    train_X_num = train_X[:, num_ind]\n",
    "    \n",
    "    valid_X_cat = valid_X[:, cat_ind]\n",
    "    valid_X_num = valid_X[:, num_ind]\n",
    "    \n",
    "    # Transform numerical variables\n",
    "    scaler = StandardScaler()\n",
    "    train_X_num = scaler.fit_transform(train_X_num)\n",
    "    valid_X_num = scaler.transform(valid_X_num)\n",
    "        \n",
    "    # Transform categorical variables\n",
    "    level_arr = [0] * train_X_cat.shape[1]\n",
    "    help_dict = list()\n",
    "    for i in range(train_X_cat.shape[1]):\n",
    "        x, help_arr, levels = encode_cat_variables(train_X_cat[:, i])\n",
    "        train_X_cat[:, i] = x\n",
    "        level_arr[i] = levels\n",
    "        help_dict.append(help_arr)\n",
    "        x, _, _ = encode_cat_variables(valid_X_cat[:, i], help_arr)\n",
    "        valid_X_cat[:, i] = x\n",
    "    \n",
    "    train_X_cat = np.array(train_X_cat).astype(int)\n",
    "    valid_X_cat = np.array(valid_X_cat).astype(int)\n",
    "    \n",
    "    return (train_X_cat, train_X_num, valid_X_cat, valid_X_num), level_arr, scaler, help_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X_cat, train_X_num, valid_X_cat, valid_X_num), level_arr, scaler, help_dict = transform_dataset(train_X, valid_X, cat_cols_idx, num_cols_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/helper_files/help_dict.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save scaler and help_dict\n",
    "model_files_dir = '../models/helper_files'\n",
    "dump(scaler, f'{model_files_dir}/scaler.joblib')\n",
    "dump(help_dict, f'{model_files_dir}/help_dict.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for tabular data.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_cat, X_num, Y):\n",
    "        self.X_cat = X_cat\n",
    "        self.X_num = X_num\n",
    "        self.Y = Y\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_cat[index], self.X_num[index], self.Y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "train_ds = TabularDataSet(train_X_cat, train_X_num, train_y)\n",
    "valid_ds = TabularDataSet(valid_X_cat, valid_X_num, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNet(nn.Module):\n",
    "    \"\"\"\n",
    "    2 layer fully connected neural network model.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_cont, num_cat, level_arr, hidden_dim=1000, hidden_dim2=1000):\n",
    "        super(TabularNet, self).__init__()\n",
    "        in_dim = num_cont + 2 * num_cat\n",
    "        self.embs = nn.ModuleList([nn.Embedding(level_arr[i], 2) for i in range(len(level_arr))])\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim2)\n",
    "        self.linear3 = nn.Linear(hidden_dim2, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "                                  \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x_cat = [self.embs[i](x_cat[:,i]) for i in range(x_cat.size(1))]\n",
    "        x_cat = torch.cat(x_cat, dim=1)\n",
    "        x_cat = self.dropout(x_cat)\n",
    "        x = torch.cat([x_cont, x_cat], dim=1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        return self.linear3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to save and retrieve trained models\n",
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to compute the consine learning rates\n",
    "def cosine_segment(start_lr, end_lr, iterations):\n",
    "    i = np.arange(iterations)\n",
    "    c_i = 1 + np.cos(i*np.pi/iterations)\n",
    "    return end_lr + (start_lr - end_lr)/2 * c_i\n",
    "\n",
    "def get_cosine_triangular_lr(max_lr, iterations, div_start=5, div_end=5):\n",
    "    min_start, min_end = max_lr/div_start, max_lr/div_end\n",
    "    iter1 = int(0.3*iterations)\n",
    "    iter2 = iterations - iter1\n",
    "    segs = [cosine_segment(min_start, max_lr, iter1), cosine_segment(max_lr, min_end, iter2)]\n",
    "    return np.concatenate(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dl, valid_dl, optimizer, max_lr=0.05, epochs=100):\n",
    "    iterations = epochs * len(train_dl)\n",
    "    idx = 0\n",
    "    best_val_r2 = 0\n",
    "    lrs = get_cosine_triangular_lr(max_lr, iterations)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        for x1, x2, y in train_dl:\n",
    "            update_optimizer(optimizer, lrs[idx])\n",
    "            x1 = x1.long() #.cuda()\n",
    "            x2 = x2.float() #.cuda()\n",
    "            y = y.unsqueeze(1).float() #.cuda()\n",
    "            y_hat = model(x1, x2)\n",
    "            loss = F.mse_loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * y.size(0)\n",
    "            total += y.size(0)\n",
    "            idx += 1\n",
    "        val_loss, val_r2 = val_metrics(model, valid_dl)\n",
    "        print(f'Train loss: {total_loss/total:.3f} \\t Valid loss: {val_loss:.3f} \\t Valid R2: {val_r2:.3f}')  \n",
    "        \n",
    "        # Saved the best trained model\n",
    "        if best_val_r2 < val_r2:\n",
    "            best_val_r2 = val_r2\n",
    "            path = f'../models/model_flights_r2_{int(100 * val_r2):02}.pth'\n",
    "            save_model(model, path)\n",
    "            print(path)\n",
    "            \n",
    "    print(f'Best valid r2: {best_val_r2:.3f}')\n",
    "    return best_val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    y_hat = []\n",
    "    ys = []\n",
    "    \n",
    "    for x1, x2, y in valid_dl:\n",
    "        batch = y.shape[0]\n",
    "        y = y.unsqueeze(1).float()\n",
    "        out = model(x1.long(), x2.float()) #.cuda()\n",
    "        loss = F.mse_loss(out, y) #.cuda(\n",
    "        sum_loss += batch * (loss.item())\n",
    "        total += batch\n",
    "        y_hat.append(out.detach().numpy()) # .cpu().\n",
    "        ys.append(y)\n",
    "    \n",
    "    y_hat = np.vstack(y_hat)\n",
    "    ys = np.vstack(ys)\n",
    "    r2 = r2_score(ys, y_hat)\n",
    "    return sum_loss/total, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cont = train_X_num.shape[1]\n",
    "num_cat = train_X_cat.shape[1]\n",
    "model = TabularNet(num_cont, num_cat, level_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 6.067 \t Valid loss: 3.895 \t Valid R2: 0.037\n",
      "../models/model_flights_r2_03.pth\n",
      "Train loss: 3.847 \t Valid loss: 3.774 \t Valid R2: 0.066\n",
      "../models/model_flights_r2_06.pth\n",
      "Train loss: 3.770 \t Valid loss: 3.687 \t Valid R2: 0.088\n",
      "../models/model_flights_r2_08.pth\n",
      "Train loss: 3.675 \t Valid loss: 3.569 \t Valid R2: 0.117\n",
      "../models/model_flights_r2_11.pth\n",
      "Train loss: 3.483 \t Valid loss: 3.220 \t Valid R2: 0.203\n",
      "../models/model_flights_r2_20.pth\n",
      "Train loss: 3.291 \t Valid loss: 3.129 \t Valid R2: 0.226\n",
      "../models/model_flights_r2_22.pth\n",
      "Train loss: 3.189 \t Valid loss: 3.009 \t Valid R2: 0.256\n",
      "../models/model_flights_r2_25.pth\n",
      "Train loss: 3.126 \t Valid loss: 2.981 \t Valid R2: 0.263\n",
      "../models/model_flights_r2_26.pth\n",
      "Train loss: 3.084 \t Valid loss: 2.924 \t Valid R2: 0.277\n",
      "../models/model_flights_r2_27.pth\n",
      "Train loss: 3.047 \t Valid loss: 2.971 \t Valid R2: 0.265\n",
      "Train loss: 3.027 \t Valid loss: 2.884 \t Valid R2: 0.287\n",
      "../models/model_flights_r2_28.pth\n",
      "Train loss: 2.962 \t Valid loss: 2.873 \t Valid R2: 0.289\n",
      "../models/model_flights_r2_28.pth\n",
      "Train loss: 2.944 \t Valid loss: 2.825 \t Valid R2: 0.301\n",
      "../models/model_flights_r2_30.pth\n",
      "Train loss: 2.921 \t Valid loss: 2.819 \t Valid R2: 0.303\n",
      "../models/model_flights_r2_30.pth\n",
      "Train loss: 2.907 \t Valid loss: 2.772 \t Valid R2: 0.314\n",
      "../models/model_flights_r2_31.pth\n",
      "Train loss: 2.894 \t Valid loss: 2.738 \t Valid R2: 0.323\n",
      "../models/model_flights_r2_32.pth\n",
      "Train loss: 2.898 \t Valid loss: 2.871 \t Valid R2: 0.290\n",
      "Train loss: 2.882 \t Valid loss: 2.766 \t Valid R2: 0.316\n",
      "Train loss: 2.875 \t Valid loss: 2.746 \t Valid R2: 0.321\n",
      "Train loss: 2.842 \t Valid loss: 2.739 \t Valid R2: 0.322\n",
      "Train loss: 2.842 \t Valid loss: 2.744 \t Valid R2: 0.321\n",
      "Train loss: 2.845 \t Valid loss: 2.724 \t Valid R2: 0.326\n",
      "../models/model_flights_r2_32.pth\n",
      "Train loss: 2.818 \t Valid loss: 2.767 \t Valid R2: 0.316\n",
      "Train loss: 2.827 \t Valid loss: 2.735 \t Valid R2: 0.323\n",
      "Train loss: 2.821 \t Valid loss: 2.707 \t Valid R2: 0.330\n",
      "../models/model_flights_r2_33.pth\n",
      "Train loss: 2.827 \t Valid loss: 2.767 \t Valid R2: 0.315\n",
      "Train loss: 2.831 \t Valid loss: 2.708 \t Valid R2: 0.330\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, weight_decay=1e-5)\n",
    "best_val = train_model(model, train_dl, valid_dl, optimizer, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
